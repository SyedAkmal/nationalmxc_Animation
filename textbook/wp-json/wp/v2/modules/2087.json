{"id":2087,"date":"2020-09-28T12:22:51","date_gmt":"2020-09-28T11:22:51","guid":{"rendered":"https:\/\/susodigital.com\/textbook\/?post_type=module&#038;p=2087"},"modified":"2020-09-28T12:22:51","modified_gmt":"2020-09-28T11:22:51","slug":"the-suso-method-server-log-file-analysis-checklist","status":"publish","type":"module","link":"https:\/\/susodigital.com\/textbook\/module\/advanced-technical-seo\/the-suso-method-server-log-file-analysis-checklist","title":{"rendered":"The SUSO Method: Server Log File Analysis Checklist"},"featured_media":0,"parent":412,"menu_order":60,"template":"","acf":{"module_quiz":"","module_time":"15","module_difficulty":{"value":"hard","label":"Hard"},"module_short_description":"<p>A checklist on how to perform server log file analysis.<\/p>\n","module_long_description":"<p>We have created a checklist on how to perform the perfect sever log file analysis to help improve your website's crawlability and indexability.<\/p>\n","video_file":false,"video_url":"","content":"<p>Now that you know what a log file is, and how a log file analysis can help you avoid wasting your crawl budget and make your website rank better, we would like to present our Server Log File Analysis Checklist which we have based on a great piece by <a href=\"https:\/\/jetoctopus.com\/the-ultimate-log-file-analysis-checklist\/\" target=\"_blank\" rel=\"nofollow noopener\">JetOctopus<\/a>.<\/p>\n<p>You can use it each time you start analysing your website\u2019s log file so that you won\u2019t forget to take actions that would increase the indexability and crawlability of your website.<\/p>\n<h2>Orphaned Pages<\/h2>\n<p>Orphaned pages are pages that aren\u2019t indexed or crawled because there is not even a single internal link that would point toward them.<\/p>\n<p>If you are running an eCommerce website, then there are probably pages of products that you are no longer selling, in which case it would be understandable if there weren\u2019t any internal links pointing toward them. You can delete them or add the noindex tag.<\/p>\n<p>However, if there are any orphaned pages with relevant and useful information, you should add internal links to make them crawlable so that users could find them.<\/p>\n<p>1. Identify orphaned pages that are of value to your website<\/p>\n<p>2. Organise these pages into relevant categories or directories (this will make it easier to add them to your website\u2019s structure)<\/p>\n<p>3. Add internal links to these pages from other relevant URLs so that they can be discovered by crawl bots.<\/p>\n<p>4. Noindex and delete any orphaned pages that have no value.<\/p>\n<h2>Pages With No Visits<\/h2>\n<p>Googlebot will send only a limited number of requests to crawl your website at a time. If the number of the pages on your website exceeds the crawl budget, which could happen due to a poor site architecture, duplicate content, or infinite spaces, some of the relevant pages won\u2019t be crawled. As a result, users won\u2019t be able to find them, which will negatively affect the rankings of your website and the user experience.<\/p>\n<p>If you want to prevent it, or at least decrease the possibility of such deviations, there are several factors that you should consider.<\/p>\n<h2>Distance From Index (DFI)<\/h2>\n<p>Even if a particular page has internal links pointing toward it, but it takes way too many clicks to get there from the homepage, it might not be crawled. In short, the fewer clicks that are required, the higher the chance that search engine spiders will crawl the page.<\/p>\n<p>You should give some serious consideration to how individual pages are interconnected so that the essential pages would be a short distance away from the homepage, which would improve their crawlability. If the pages with the most useful content require many clicks to get there, consider linking to them from a homepage or another page with a low DFI.<\/p>\n<p>1. Ensure pages that are important are no more than 3 clicks away from the homepage.<\/p>\n<p>2. Identify important pages that have not been crawled and add links to them from pages with a smaller DFI.<\/p>\n<h2>Inlinks<\/h2>\n<p>If there are external and internal links pointing toward a particular page, it is a sign for search engine crawlers that it might be useful to the users. As a result, it has a serious effect on the crawlability and rankings of the page.<\/p>\n<p>1. Identify pages of importance that have not been crawled.<\/p>\n<p>2. Increase the number of inlinks pointing towards these pages. Alternatively, you could outreach to other websites &#8211; external links would be fine too.<\/p>\n<h2>Wordcount<\/h2>\n<p>Although it isn\u2019t necessarily true that pages with more content are better, you should generally avoid pages with less than 500 words, as they are less likely to be crawled. Not every page needs to have 1500+ words articles, but consider whether your audience would benefit from a longer piece of content. Alternatively, you could combine a few shorter pages into a single one.<\/p>\n<h2>Title Tags<\/h2>\n<p>Titles help both your audience and crawler bots understand in a short amount of time what your content is really about. Issues with title tags can prevent search engine bots from properly understanding the context of the text, decreasing the crawlability, indexability, and rankings of the page.<\/p>\n<p>From an SEO perspective, the worst-case scenario is if the title tags are empty, which leads to confusion among your readers and robots alike. However, search engine crawlers might also get confused if the titles are duplicated. Although it\u2019s a less serious problem, it could be harmful to your website as well.<\/p>\n<p>1. Identify pages with empty title tags and optimise accordingly<\/p>\n<p>2. Identify pages with duplicate title tags and make sure to replace with unique tags<\/p>\n<h2>Crawl Frequency<\/h2>\n<p>If you would like to make changes to the internal linking on your website, we recommend checking which pages are the most and the least frequently visited by crawlers. With the JetOctopus Log Analyzer, you can do so by clicking on charts in the \u201cPages by Bot Visits\u201d section.<\/p>\n<p>If you discover that some of the pages with the most useful content to the users are visited by bots very infrequently, it could be a sign that there are some issues that need to be fixed. As we have mentioned previously, the page might require too many clicks from the homepage, there could be not enough inlinks pointing toward it, or the page might lack title tags.<\/p>\n<h2>Non-Indexable Pages<\/h2>\n<p>Even though some pages are non-indexable, they are visited by search engine crawlers. It includes:<\/p>\n<div class=\"list-styles list-styles__circle list-styles__circle--primary-color\"><ul>\n<li>Pages with non-200 status code<\/li>\n<li>Non-canonical pages<\/li>\n<li>Pages that are blocked both in robots meta tag and X-Robots-Tag<\/li>\n<\/ul>\n<\/div>\n<p>Why are they being crawled? Usually, that\u2019s because other pages on your website point toward them. It is especially common for the non-canonical pages to be mistakenly linked to as canonical, though it also happens if there are multiple language versions.<\/p>\n<p>As a result, crawlers will visit non-indexable pages, wasting the crawl budget, which could lead to other important pages not being visited by robots.<\/p>\n<p>1. Remove internal linking to pages that are non-indexable<\/p>\n<p>2. Check if all the links point toward canonical pages or the appropriate language version.<\/p>\n<h2>HTTP Response Status Codes<\/h2>\n<p>Next, let\u2019s focus on HTTP response status codes which we have already explored in the previous module. We have seen how some of them can have a considerable effect on your website\u2019s crawlability and indexability, which is why you should monitor their numbers if you want to ensure your website\u2019s health.<\/p>\n<h3>5xx Errors<\/h3>\n<p>5xx errors indicate that there are problems with the server\u2019s response time, which is extremely bad from an SEO standpoint. It makes it harder for search engine bots to crawl your website, but it also leads to poor user experience.<\/p>\n<p>To check if there are any 5xx errors presented on your website, go to the Health and Bot Dynamics Dashboards in JetOctopus Log Analyzer. If you discover that your website has a high number of 5xx errors, you might need to invest in better servers.<\/p>\n<h3>3xx Status Codes<\/h3>\n<p>3xx status codes indicate that a redirection is required for a user to arrive at the requested destination. If you discover that they are visited by crawlers, we recommend checking if:<\/p>\n<div class=\"list-styles list-styles__circle list-styles__circle--primary-color\"><ul>\n<li>They are linked to other pages.<\/li>\n<li>They have a canonical tag.<\/li>\n<li>Other language versions point toward them.<\/li>\n<li>They can be found in XML sitemaps.<\/li>\n<li>There are issues with redirect chains.<\/li>\n<\/ul>\n<\/div>\n<h3>4xx Status Codes<\/h3>\n<p>4xx status codes can hurt the website\u2019s crawlability, indexability, and rankings, as they indicate that the requested page couldn\u2019t be found. Regardless of whether the page is permanently or temporarily missing, it will lead to a waste of crawl budget. To fix this issue, you should check the exact same things as in the case of 3xx status codes.<\/p>\n<h2>Page Load Time<\/h2>\n<p>How long it takes for a page to load has a huge impact on its crawlability, but it also affects user experience. If it takes too much time to load the page, the visitors will simply leave. That\u2019s why monitoring the load time of the pages on your website is essential from an SEO perspective.<\/p>\n<p>Refer to our Site Speed Optimisation from earlier in this module to learn how to improve your page load times.<\/p>\n<h2>Mobile-First Indexing<\/h2>\n<p>Mobile devices account for the majority of traffic, which is why currently, Google uses the mobile version of your pages for indexing. That\u2019s why you should ensure that mobile users don\u2019t encounter any issues while visiting your website. For a refresher on mobile-first indexing, you can head over to your dedicated chapter from this module.<\/p>\n<h2>User Experience<\/h2>\n<p>The final aspect that we want to mention is the user experience. You can use log files to discover how users navigate on your website and what problems they often encounter.<\/p>\n<p>Consider which pages are most often visited, and which ones are skipped. If some of the pages with excellent content are rarely visited, you might need to rework the internal linking on your website, or there might be some technical issues that would require your attention.<\/p>\n<h2>Conclusion<\/h2>\n<p>Log file analysis can help you diagnose problems on your website that could be hurting its crawlability and rankings. We hope that this checklist will help you fix any issues you might encounter.<\/p>\n<p>&nbsp;<\/p>\n"},"_links":{"self":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/2087"}],"collection":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules"}],"about":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/types\/module"}],"version-history":[{"count":3,"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/2087\/revisions"}],"predecessor-version":[{"id":2090,"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/2087\/revisions\/2090"}],"up":[{"embeddable":true,"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/412"}],"wp:attachment":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/media?parent=2087"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}