{"id":1416,"date":"2020-08-10T18:00:56","date_gmt":"2020-08-10T17:00:56","guid":{"rendered":"https:\/\/susodigital.com\/textbook\/?post_type=module&#038;p=1416"},"modified":"2020-08-10T13:41:42","modified_gmt":"2020-08-10T12:41:42","slug":"natural-language-processing","status":"publish","type":"module","link":"https:\/\/susodigital.com\/textbook\/module\/information-architecture\/natural-language-processing","title":{"rendered":"Natural Language Processing"},"featured_media":0,"parent":408,"menu_order":45,"template":"","acf":{"module_quiz":"","module_time":"40","module_difficulty":{"value":"hard","label":"Hard"},"module_short_description":"<p>A detailed look at Natural Language Processing and Google BERT.<\/p>\n","module_long_description":"<p>In this chapter we dive into the Google BERT algorithm which uses the concept of Natural Language Processing to understand the sentiment and search intent of search queries.<\/p>\n","video_file":false,"video_url":"","content":"<p>Natural Language Processing (or NLP) refers to a branch of artificial intelligence and machine learning with the goal of helping computers to understand <strong><dfn title=\"the way we humans communicate with each other\">natural language<\/dfn><\/strong>.<\/p>\n<p>You\u2019ve probably come across the power of NLP without even realising it.<\/p>\n<p>Ever had to \u201cspeak\u201d to a robot when dealing with customer service on the phone or used a personal assistant application like Siri or OK Google on your phone?<\/p>\n<p>Ever wondered how word processing tools like Microsoft Word and Grammarly are able to check the grammatical errors on your text?<\/p>\n<p>They\u2019re all using NLP to understand what you\u2019ve said or typed.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1420\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing.jpg\" alt=\"\" width=\"1780\" height=\"1286\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing.jpg 1780w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing-1536x1110.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing-55x40.jpg 55w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing-111x80.jpg 111w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing-42x30.jpg 42w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing-83x60.jpg 83w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing-33x24.jpg 33w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing-66x48.jpg 66w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing-25x18.jpg 25w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Natural-language-processing-50x36.jpg 50w\" sizes=\"(max-width: 1780px) 100vw, 1780px\" \/>\n<p>As you\u2019ve probably guessed by now, with the help of semantic web technologies and machine-learning, NLP is also a HUGE part of organic search.<\/p>\n<p>However, before we unlock the enigma that is NLP, let\u2019s take a look at the equally mystifying BERT &#8211; which is <a href=\"https:\/\/blog.google\/products\/search\/search-language-understanding-bert\" target=\"_blank\" rel=\"nofollow noopener\">described by Google<\/a> as their \u201cbiggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search\u201d.<\/p>\n<h2>Google BERT Algorithm<\/h2>\n<p>Announced by Google in October 2019, BERT is an algorithm update that allows bots to understand the sentiment and search intent of search queries significantly better.<\/p>\n<p>According to a statement from Google, a fully rolled out update impacted 10% of all searches.<\/p>\n<p>Fast forward three months to January 2020, and Google has already followed through with a core algorithm update called January 2020 Core Update.<\/p>\n<blockquote class=\"twitter-tweet\">\n<p dir=\"ltr\" lang=\"en\">Later today, we are releasing a broad core algorithm update, as we do several times per year. It is called the January 2020 Core Update. Our guidance about such updates remains as we\u2019ve covered before. Please see this blog post for more about that:<a href=\"https:\/\/t.co\/e5ZQUA3RC6\">https:\/\/t.co\/e5ZQUA3RC6<\/a><\/p>\n<p>\u2014 Google SearchLiaison (@searchliaison) <a href=\"https:\/\/twitter.com\/searchliaison\/status\/1216752087515586560?ref_src=twsrc%5Etfw\">January 13, 2020<\/a><\/p><\/blockquote>\n<p><script async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"><\/script><\/p>\n<p>There are hundreds if not thousands of smaller updates to Google\u2019s algorithms every year, but the fact that Google gave the SEO world a heads up before rolling this one out, highlights its importance.<\/p>\n<p>Although this tweet gives no insight into the specifics of what the algorithm focused on, the timing of this core update suggests that BERT and NLP have a big part in it.<\/p>\n<p>So, let\u2019s start off by taking a closer look at BERT, as it\u2019s the key we\u2019ll use to unlock the concept of NLP and these latest changes from Google.<\/p>\n<h3>What is BERT?<\/h3>\n<p>Originally introduced in 2018, BERT (which stands for Bidirectional Encoder Representations from Transformers) is an <strong><dfn title=\"open-sourced software is software whose source code is publicly available under licenses. For example, anyone can use BERT to train their own language processing system.\">open-sourced<\/dfn><\/strong> technique based on <strong><dfn title=\"neural networks are machine learning algorithms that recognise patterns based on training data. Application include categorising image content, handwriting recognition and even predicting trends in the financial markets.\">neural networks<\/dfn><\/strong> for natural language processing pre-training using the plain text corpus of the English Wikipedia.<\/p>\n<p>That\u2019s right, the entire English Wikipedia!<\/p>\n<p>Let\u2019s look at one of the ways BERT helps understand contextual differences in language.<\/p>\n<p>In the sentences \u201cI play the bass guitar\u201d and \u201cI eat sea bass\u201d, the word \u201cbass\u201d has two different meanings; it\u2019s a polysemous word. Whilst this difference is more obvious to us humans, it\u2019s not so simple for machines.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1423\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example.jpg\" alt=\"\" width=\"1685\" height=\"1290\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example.jpg 1685w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example-1536x1176.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example-52x40.jpg 52w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example-104x80.jpg 104w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example-39x30.jpg 39w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example-78x60.jpg 78w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example-31x24.jpg 31w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example-63x48.jpg 63w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example-24x18.jpg 24w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-processing-example-47x36.jpg 47w\" sizes=\"(max-width: 1685px) 100vw, 1685px\" \/>\n<p>Likewise, BERT is able to comprehend other types of <strong><dfn title=\"in the field of linguistics, ambiguity is at the sentence level as opposed to word level i.e. words with multiple meanings create ambiguous sentences and phrases that are difficult for machines to understand.\">lexically ambiguous<\/dfn><\/strong> words and phrases such as <strong><dfn title=\"words with two or more significantly different meanings i.e. rose (noun) is a flower, but rose (verb) means to rise up.\">homonyms<\/dfn><\/strong>, <strong><dfn title=\"words that are spelled differently, but sound the same i.e. &quot;there&quot; and &quot;their&quot;.\">homophones<\/dfn><\/strong>, <strong><dfn title=\"words that are spelled the same, but have different meanings i.e. rose (noun) is a flower, but rose (verb) means to rise up.\">homographs<\/dfn><\/strong>, <strong><dfn title=\"words that have a similar or the same meanings i.e. breathtaking and astonishing.\">synonyms<\/dfn><\/strong> and more.<\/p>\n<p>To summarise: BERT distinguishes subtle nuances in language to provide more relevant search results.<\/p>\n<h3>How Does BERT Work?<\/h3>\n<p>To gain a better understanding of how BERT works, we need to look at what the acronym actually stands for.<\/p>\n<h4>B: Bi-directional<\/h4>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1426\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional.jpg\" alt=\"\" width=\"1644\" height=\"1017\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional.jpg 1644w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-1536x950.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-65x40.jpg 65w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-129x80.jpg 129w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-48x30.jpg 48w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-97x60.jpg 97w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-39x24.jpg 39w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-78x48.jpg 78w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-29x18.jpg 29w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-58x36.jpg 58w\" sizes=\"(max-width: 1644px) 100vw, 1644px\" \/>\n<p>The key to begin understanding how BERT and in turn how NLP works, is by looking at the first part of its name: Bidirectional.<\/p>\n<p>The dictionary definition for \u201cbidirectional\u201d refers to two directions of a particular function or reaction.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1427\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-definition.png\" alt=\"\" width=\"890\" height=\"202\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-definition.png 890w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-definition-176x40.png 176w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-definition-352x80.png 352w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-definition-132x30.png 132w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-definition-264x60.png 264w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-definition-106x24.png 106w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-definition-211x48.png 211w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-definition-79x18.png 79w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Bi-directional-definition-159x36.png 159w\" sizes=\"(max-width: 890px) 100vw, 890px\" \/>\n<p>This is the equivalent of Google calculating the meaning of a word of phrase by looking at the preceding and succeeding content.<\/p>\n<p>The other part of the definition refers to the ability to learn (or react).<\/p>\n<p>What makes BERT such a breakthrough, is its ability to learn language models based on an entire set of training data i.e. it looks at the entire set of words in a sentence or search query rather than simply moving through the sequence of words from left-to-right.<\/p>\n<p>Putting the two together: BERT uses bidirectional training to learn the context of a word based on surrounding words rather than just the words that immediately precede or follow it.<\/p>\n<p>Google <a href=\"https:\/\/ai.googleblog.com\/2018\/11\/open-sourcing-bert-state-of-art-pre.html\" target=\"_blank\" rel=\"nofollow noopener\">refers<\/a> to BERT as <em>\u201cdeeply bidirectional\u201d<\/em> &#8211; it can \u201csee\u201d every word in a sentence simultaneously as well as understand how they impact the context of the other words in the sentence, rather than one at a time (unidirectional).<\/p>\n<p>Unidirectional models try to predict the next word in a sequence (which is effective because they cannot \u201csee\u201d what the next word is), however this does not work for bidirectional models like MLM because it would enable it to indirectly \u201csee\u201d the word that it was trying to guess.<\/p>\n<p><strong>Masked Model Learning<\/strong><\/p>\n<p>Google achieves bidirectionality by training BERT using what\u2019s called a Masked Learning Model (MLM).<\/p>\n<p>By adopting a masked learning model, Google was able to train the natural language processors by \u201cmasking out some of the words in the input and then condition each word bidirectionally to predict the masked words\u201d.<\/p>\n<p>Let\u2019s take the bizarre but illustrative sentence: \u201cI love how you love that she loves that.\u201d<\/p>\n<p>As the sentence progresses, the part of text which the word \u201clove\u201d relates to changes along with the context of each use.<\/p>\n<p>A unidirectional model would be unable to determine the full context of this sentence because it would only be able to \u201csee\u201d the preceding and succeeding words. In other words, if it looked at the first occurrence of \u201clove\u201d, it would only see \u201cI\u201d and \u201chow\u201d, but would not be able to see the rest of the sentence and in turn may miss the shape shifting context in this sentence.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1428\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning.jpg\" alt=\"\" width=\"1947\" height=\"783\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning.jpg 1947w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-1536x618.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-1920x772.jpg 1920w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-99x40.jpg 99w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-199x80.jpg 199w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-75x30.jpg 75w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-149x60.jpg 149w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-60x24.jpg 60w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-119x48.jpg 119w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-45x18.jpg 45w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-90x36.jpg 90w\" sizes=\"(max-width: 1947px) 100vw, 1947px\" \/>\n<p>BERT\u2019s bidirectionality however, enables it to see the entire sentence from both directions &#8211; i.e. it sees each word at the same time &#8211; and as a result is able to consider the entire sentence\u2019s context, like a human.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1429\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2.jpg\" alt=\"\" width=\"1947\" height=\"798\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2.jpg 1947w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2-1536x630.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2-1920x787.jpg 1920w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2-98x40.jpg 98w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2-195x80.jpg 195w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2-73x30.jpg 73w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2-146x60.jpg 146w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2-59x24.jpg 59w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2-117x48.jpg 117w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2-44x18.jpg 44w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Masked-model-learning-2-88x36.jpg 88w\" sizes=\"(max-width: 1947px) 100vw, 1947px\" \/>\n<p><strong>Textual Entailment<\/strong><\/p>\n<p>Taking things a step further, BERT is also trained to perform textual entailment, which is a more sophisticated way of saying that it is able to \u201cpredict the next sentence\u201d.<\/p>\n<p>In other words, given two sentences, BERT is able to predict whether or not the second sentence fits within the context of the first sentence.<\/p>\n<p>In the examples below, BERT would be able to predict that the first set of sentences are contextually relevant, whereas the second pair of sentences are not.<\/p>\n<p>Cricket is the best sport in the world.<\/p>\n<p>My favorite player is Ben Stokes.<\/p>\n<p>IsNextSentence<\/p>\n<p>Cricket is the best sport in the world.<\/p>\n<p>Water is a liquid.<\/p>\n<p>IsNotNextSentence<\/p>\n<h4>ER: Encoder Representations<\/h4>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1430\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation.jpg\" alt=\"\" width=\"1653\" height=\"1009\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation.jpg 1653w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation-1536x938.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation-66x40.jpg 66w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation-131x80.jpg 131w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation-49x30.jpg 49w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation-98x60.jpg 98w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation-39x24.jpg 39w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation-79x48.jpg 79w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation-29x18.jpg 29w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Encoder-representation-59x36.jpg 59w\" sizes=\"(max-width: 1653px) 100vw, 1653px\" \/>\n<p>The Encoder Representations part of BERT\u2019s name refers to its transformer architecture.<\/p>\n<p>The sentence input is translated to representative models of the words\u2019 meaning, this is known as the <strong>encoder<\/strong>.<\/p>\n<p>The processed text output that is contextualised is known as the <strong>decoder<\/strong>.<\/p>\n<p>Essentially, it\u2019s an in-and-out situation where whatever is encoded (the text), is then decoded (the meaning).<\/p>\n<h4>T: Transformers<\/h4>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1431\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers.jpg\" alt=\"\" width=\"1644\" height=\"1000\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers.jpg 1644w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers-1536x934.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers-66x40.jpg 66w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers-132x80.jpg 132w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers-49x30.jpg 49w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers-99x60.jpg 99w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers-39x24.jpg 39w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers-79x48.jpg 79w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers-30x18.jpg 30w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Transformers-59x36.jpg 59w\" sizes=\"(max-width: 1644px) 100vw, 1644px\" \/>\n<p>The final component of BERT is the transformers.<\/p>\n<p>As we\u2019ve already established, one of the main issues that NLP poses, is with not being able to understand the context of a particular word within a sentence.<\/p>\n<p>We\u2019ve already seen how it can be pretty easy for a machine to lose track of a subject within a sentence when varying pronouns are used.<\/p>\n<p>So, one of the main purposes of the transformer, is to focus (or \u201cfixate\u201d) on both the meaning of any pronouns used, on top of the meaning of all other words used too. These are then used to try to piece the sentence together and contextualise the subject of the conversation or sentence &#8211; this is known as <strong>coreference resolution<\/strong>.<\/p>\n<p>We\u2019ve seen that one of the mechanisms that helps do this is MLM.<\/p>\n<h3>The Relationship Between BERT and NLP<\/h3>\n<p>We now know that BERT is built on two main components:<\/p>\n<p>1. Data &#8211; the pre trained input models (sets of data)<\/p>\n<p>2. Methodology &#8211; a defined way to learn and use those models<\/p>\n<p>Providing BERT with an input set of data to learn from is all well and good, but you also need to define a process for BERT to be able to correctly understand and interpret this data.<\/p>\n<p>This is where NLP comes into play.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1432\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP.jpg\" alt=\"\" width=\"1725\" height=\"1056\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP.jpg 1725w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP-1536x940.jpg 1536w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP-65x40.jpg 65w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP-131x80.jpg 131w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP-49x30.jpg 49w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP-98x60.jpg 98w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP-39x24.jpg 39w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP-78x48.jpg 78w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP-29x18.jpg 29w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-BERT-and-NLP-59x36.jpg 59w\" sizes=\"(max-width: 1725px) 100vw, 1725px\" \/>\n<h3>The Impact of BERT and NLP On SEO<\/h3>\n<p>With billions of searches being made every day, understanding language has always been at the core of Search.<\/p>\n<p>It\u2019s the search engine\u2019s job to work out what you\u2019re searching for and provide helpful information from the web &#8211; regardless of what combination of words or spellings is used in the search query.<\/p>\n<p>If we go back to why BERT exists in the first place (to improve machines\u2019 understanding of human language) we can see how this all fits into place from an SEO perspective.<\/p>\n<p>Natural language processing changes the way search engines understand queries at word level and through BERT, Google is able to resolve linguistic ambiguities in natural language by looking at each word in a sentence simultaneously.<\/p>\n<p>The implications of BERT and NLP on SEO are humongous!<\/p>\n<p>Let\u2019s dive in and see exactly how these two powerful mechanisms impact SEO.<\/p>\n<h4>Contextualised SERPs<\/h4>\n<p>A change in the way search engines understand queries naturally impacts the search results themselves.<\/p>\n<p><strong>Featured Snippets<\/strong><\/p>\n<p>One of the areas in the SERPs that is affected, is the featured snippets.<\/p>\n<p>Let\u2019s take a look at an example of a featured snippet that Google themselves have provided for the query \u201cparking on a hill with no curb\u201d.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1433\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Parking-on-a-hill.jpg\" alt=\"\" width=\"800\" height=\"463\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Parking-on-a-hill.jpg 800w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Parking-on-a-hill-69x40.jpg 69w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Parking-on-a-hill-138x80.jpg 138w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Parking-on-a-hill-52x30.jpg 52w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Parking-on-a-hill-104x60.jpg 104w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Parking-on-a-hill-41x24.jpg 41w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Parking-on-a-hill-83x48.jpg 83w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Parking-on-a-hill-31x18.jpg 31w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Parking-on-a-hill-62x36.jpg 62w\" sizes=\"(max-width: 800px) 100vw, 800px\" \/>\n<p>In the past, we can see that this query wasn\u2019t quite understood correctly by Google\u2019s systems. Google said that they ignored the word \u201cno\u201d and placed \u201ctoo much importance\u201d on the word \u201ccurb\u201d.<\/p>\n<p>Not being able to understand this critical word resulted in inappropriate results where Google would return results for \u201cparking on a hill with a curb\u201d.<\/p>\n<p><strong>Search Results for Textual Search Queries<\/strong><\/p>\n<p>Likewise, for queries that are more sophisticated or conversational, the meaning of propositions or stop-words (i.e. \u201cand\u201d, \u201cto\u201d, \u201cin\u201d and \u201cfor\u201d) are more difficult to assess.<\/p>\n<p>Here is an example from Google which shows how BERT\u2019s improved understanding of the stop-word \u201cto\u201d provided better search results.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1435\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Brazillian-traveling-to-US.png\" alt=\"\" width=\"800\" height=\"432\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Brazillian-traveling-to-US.png 800w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Brazillian-traveling-to-US-74x40.png 74w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Brazillian-traveling-to-US-148x80.png 148w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Brazillian-traveling-to-US-56x30.png 56w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Brazillian-traveling-to-US-111x60.png 111w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Brazillian-traveling-to-US-44x24.png 44w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Brazillian-traveling-to-US-89x48.png 89w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Brazillian-traveling-to-US-33x18.png 33w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Brazillian-traveling-to-US-67x36.png 67w\" sizes=\"(max-width: 800px) 100vw, 800px\" \/>\n<p>The query is about a Brazilian travelling to the U.S.A and not the other way around. However, Google returned results about U.S. citizens travelling to Brazil.<\/p>\n<p>With BERT applied, Google is able to understand the importance of the relationship between the word \u201cto\u201d and the other words in the query better and in turn, is able to provide a much more relevant result to the user.<\/p>\n<p><strong>Improved Voice Search<\/strong><\/p>\n<p>BERT\u2019s ability to be able to detect ambiguities in more conversational queries also extends to voice search.<\/p>\n<p>This means that you can \u201cspeak\u201d to Google as if you were speaking to an actual human.<\/p>\n<p>For example, if you were to ask Google Assistant \u201cWhat is Semantic SEO?\u201d, instead of seeing a list of search results, Google will be able to answer your query using natural language.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1436\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Voice-search.png\" alt=\"\" width=\"196\" height=\"300\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Voice-search.png 196w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Voice-search-26x40.png 26w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Voice-search-52x80.png 52w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Voice-search-20x30.png 20w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Voice-search-39x60.png 39w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Voice-search-16x24.png 16w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Voice-search-31x48.png 31w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Voice-search-12x18.png 12w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Voice-search-24x36.png 24w\" sizes=\"(max-width: 196px) 100vw, 196px\" \/>\n<h4>Benefits to International Search<\/h4>\n<p>BERT is already being applied to Search across the world &#8211; this will present major benefits to the area of international search.<\/p>\n<p>The beauty of BERT is that it is able to \u201ctake learnings from one language and apply them to others\u201d which means that Google is able to take models that learn from improvements in English, and effectively transfer these learnings to other languages.<\/p>\n<h4>Structured Data Markup<\/h4>\n<p>With the combined power of BERT and NLP, any information you provide to Google about the content on a web page or your website in general, effectively has more meaning than ever before.<\/p>\n<p>This makes marking up your content with structured data even more important and beneficial. In recent times, Google has promoted the usage of structured data by providing <a href=\"https:\/\/developers.google.com\/search\/case-studies\/overview\" target=\"_blank\" rel=\"nofollow noopener\">several case studies<\/a> for businesses whose Search improved based on the quality of their schema.org markup.<\/p>\n<p>Eventbrite, the world\u2019s largest event technology platform reaped the rewards with \u201croughly a 100-percent increase in the typical year-over-year growth of traffic\u201d when they marked up their event listings using the Event structured data markup.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1438\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-scaled.jpg\" alt=\"\" width=\"1245\" height=\"2560\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-scaled.jpg 1245w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-747x1536.jpg 747w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-996x2048.jpg 996w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-19x40.jpg 19w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-39x80.jpg 39w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-15x30.jpg 15w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-29x60.jpg 29w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-12x24.jpg 12w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-23x48.jpg 23w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-9x18.jpg 9w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Case-study-18x36.jpg 18w\" sizes=\"(max-width: 1245px) 100vw, 1245px\" \/>\n<p>BERT\u2019s ability to understand natural language better enables it to grasp the information provided in the structured data markup, which in turn allows Google to index these pages more accurately so that they appear in the SERPs for more relevant terms.<\/p>\n<h2>How Does NLP Enhance Search Quality?<\/h2>\n<p>If <a href=\"https:\/\/www.blog.google\/products\/search\/search-language-understanding-bert\/\" target=\"_blank\" rel=\"nofollow noopener\">15% of all searches made online<\/a> have never been seen by Google before, how is Google able to anticipate the intent behind the search term and provide relevant results?<\/p>\n<p>The key to providing relevant results for unseen queries, is to understand language.<\/p>\n<p>In order to understand language, we\u2019ve already established that context is everything. On top of context, machines also need to be able to glean the <u>sentiment<\/u> of a query, be able to identify the subject (<u>entity<\/u>) of the queries and finally, the importance (<u>salience<\/u>) of the query.<\/p>\n<p>You can see how Google \u201creads\u201d your content by putting it into its <a href=\"https:\/\/cloud.google.com\/natural-language\" target=\"_blank\" rel=\"nofollow noopener\">Cloud Natural Language API Demo<\/a> which uses machine learning AI and NLP to understand the text.<\/p>\n<h3>What is Sentiment in NLP?<\/h3>\n<p>Google needs to be able to identify what the underlying tone of content is in order to present the most relevant results for a particular search query.<\/p>\n<p>Like emotion, sentiment can be analysed based on its <u>polarity<\/u>, i.e. it can be either positive, negative or neutral.<\/p>\n<p><strong>Positive Sentiment<\/strong> refers to when a topic is being described or discussed in a favourable tone and usually includes positive words like \u201cawesome\u201d, \u201clegend\u201d, \u201cbrilliant\u201d etc.<\/p>\n<p>In Google\u2019s Natural Language API, the sentiment is considered as positive if the value is between 0.25 and 1.0.<\/p>\n<p><strong>Negative Sentiment<\/strong>, as you have probably already guessed, uses unfavorable language such as \u201cbad\u201d, \u201chate\u201d, \u201cupsetting\u201d etc.<\/p>\n<p>Negative sentiment falls within the range of -1.0 and -0.25<\/p>\n<p><strong>Neutral<\/strong> content contains a mix of both positive and negative language.<\/p>\n<p>The resulting score range for neutral pieces of content falls within -0.25 and 0.25.<\/p>\n<p>Google\u2019s algorithm calculates the sentiment value based on each subsection of the content as opposed to the entire page.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1439\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Sentiment-score-range.jpg\" alt=\"\" width=\"1522\" height=\"603\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Sentiment-score-range.jpg 1522w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Sentiment-score-range-101x40.jpg 101w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Sentiment-score-range-202x80.jpg 202w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Sentiment-score-range-76x30.jpg 76w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Sentiment-score-range-151x60.jpg 151w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Sentiment-score-range-61x24.jpg 61w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Sentiment-score-range-121x48.jpg 121w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Sentiment-score-range-45x18.jpg 45w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Sentiment-score-range-91x36.jpg 91w\" sizes=\"(max-width: 1522px) 100vw, 1522px\" \/>\n<h3>What is The Entity in NLP?<\/h3>\n<p>An entity represents the subject or named object in a piece of text.<\/p>\n<p>Examples of Google\u2019s entity categories include persons, consumer goods, events, locations, numbers, organizations and more.<\/p>\n<p>Google\u2019s Natural Language AI uses NLP to identify and evaluate these entities within your content which enables it to obtain useful information that helps satisfy the user\u2019s intent and present better search results.<\/p>\n<p>Naturally, Google looks at nouns (aka naming words) and noun phrases (i.e. \u201cthe website ranks well\u201d) to identify, classify and categorise entities. This also includes names (proper nouns), in fact, a <a href=\"https:\/\/storage.googleapis.com\/pub-tools-public-publication-data\/pdf\/42235.pdf\" target=\"_blank\" rel=\"nofollow noopener\">research paper published by Google in 2014<\/a> focused primarily on people\u2019s names.<\/p>\n<p>Using Google\u2019s Natural Language API demo, you can see exactly how entities are defined by the AI. We fed an article from the BBC on England cricketer Adil Rashid and we can see that the demo recognised that the following words all referred to the same person:<\/p>\n<div class=\"list-styles list-styles__circle list-styles__circle--primary-color\"><ul>\n<li>Adil Rashid (named)<\/li>\n<li>Rashid (named)<\/li>\n<li>Spinner (nominal)<\/li>\n<\/ul>\n<\/div>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1441\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-API-demo.png\" alt=\"\" width=\"869\" height=\"737\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-API-demo.png 869w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-API-demo-47x40.png 47w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-API-demo-94x80.png 94w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-API-demo-35x30.png 35w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-API-demo-71x60.png 71w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-API-demo-28x24.png 28w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-API-demo-57x48.png 57w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-API-demo-21x18.png 21w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-API-demo-42x36.png 42w\" sizes=\"(max-width: 869px) 100vw, 869px\" \/>\n<p>Google\u2019s AI was able to understand that the word \u201cspinner\u201d was being used to refer to Adil Rashid.<\/p>\n<p>Interestingly, it also classified \u201cEngland\u201d as an Organisation as opposed to a Location, as it understood that the article is talking about the England Cricket Team instead of the country.<\/p>\n<p>Moreover, the AI also classifies the content into different categories, in this case \/Sports\/Team Sports\/Cricket and \/News\/Sports News.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1442\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Category-example.png\" alt=\"\" width=\"836\" height=\"459\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Category-example.png 836w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Category-example-73x40.png 73w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Category-example-146x80.png 146w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Category-example-55x30.png 55w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Category-example-109x60.png 109w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Category-example-44x24.png 44w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Category-example-87x48.png 87w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Category-example-33x18.png 33w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Category-example-66x36.png 66w\" sizes=\"(max-width: 836px) 100vw, 836px\" \/>\n<p>You can find a full list of categories <a href=\"https:\/\/cloud.google.com\/natural-language\/docs\/categories\" target=\"_blank\" rel=\"nofollow noopener\">here<\/a>.<\/p>\n<h3>What is Salience in NLP?<\/h3>\n<p>In linguistics, the term <strong>salience<\/strong> refers to the prominence or importance of a word or phrase within a particular piece of content. In the case of NLP, it represents the importance of the entity within a piece of text.<\/p>\n<p>As with sentiment, Google assigns each entity with a score based on its perceived salience in relation to the analysed text. This ranges from 0.0 to 1.0 where the higher the salience value, the more important and relevant the entity is for the page\u2019s subject. This score is a prediction on what a human being would consider to be the most important entities within the same text.<\/p>\n<p>For example, in our example from before, we can see that Google gives \u201cAdil Rashid\u201d the highest salience score of 0.69, which comes as no surprise, considering the article is about him.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1443\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score.png\" alt=\"\" width=\"835\" height=\"744\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score.png 835w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-45x40.png 45w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-90x80.png 90w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-34x30.png 34w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-67x60.png 67w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-27x24.png 27w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-54x48.png 54w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-20x18.png 20w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-40x36.png 40w\" sizes=\"(max-width: 835px) 100vw, 835px\" \/>\n<p>Google bases this score on several numerical factors:<\/p>\n<div class=\"list-styles list-styles__circle list-styles__circle--primary-color\"><ul>\n<li>The position of the entity within the text, i.e. the location of the first time the entity is discovered in the text &#8211; generally, entities placed closer to the beginning of the article are seen as more salient.<\/li>\n<li>The grammatical role of the entity i.e. the relationship between the subject and the object within a sentence.<\/li>\n<li>The entity\u2019s linguistic links to other parts of the sentence, i.e. the most salient entities are usually grammatically linked within the sentence, you can see how Google breaks down each sentence\u2019s words in the <a href=\"https:\/\/cloud.google.com\/natural-language\" target=\"_blank\" rel=\"nofollow noopener\">API Demo by clicking on the Syntax tab<\/a>.<\/li>\n<li>A tally of the named, nominal and pronominal references to the entity (i.e. \u201cAdil Rashid\u201d, \u201cspinner\u201d, \u201che\u201d) &#8211; naturally, the more an entity is mentioned, the higher salience Google is likely to give it.<\/li>\n<\/ul>\n<\/div>\n<h4>Entity Graphs and Indexing<\/h4>\n<p>In the 2014 paper published by Dunietz and Gillick introduce the concept of an entity graph.<\/p>\n<p>The entity graph is based on Google\u2019s existing PageRank calculation which determines a page\u2019s authority based on its incoming links. Google uses its own Knowledge Graph as its database of connected entities.<\/p>\n<p>We\u2019ve already encountered Google\u2019s Knowledge Graph.<\/p>\n<p>As a refresher, here\u2019s what the knowledge graph for England cricket Ben Stokes looks like.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1444\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Knowledge-graph.png\" alt=\"\" width=\"409\" height=\"724\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Knowledge-graph.png 409w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Knowledge-graph-23x40.png 23w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Knowledge-graph-45x80.png 45w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Knowledge-graph-17x30.png 17w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Knowledge-graph-34x60.png 34w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Knowledge-graph-14x24.png 14w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Knowledge-graph-27x48.png 27w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Knowledge-graph-10x18.png 10w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Knowledge-graph-20x36.png 20w\" sizes=\"(max-width: 409px) 100vw, 409px\" \/>\n<p>The entity graph\u2019s primary purpose is to simulate the wider context for entities in a given piece of content, which humans would subconsciously draw upon. This is because as humans, we usually already know something about at least some of the entities involved, and importantly, we also have some idea of their salience.<\/p>\n<p>Essentially, this enables the AI to adapt its salience scores for an entity based on its connections to other entities within the text.<\/p>\n<p>In our example article about Adil Rashid, Yorkshire is given a higher salience score than West Indies even though both are mentioned only once within the text that was analysed.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1445\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-example.png\" alt=\"\" width=\"630\" height=\"782\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-example.png 630w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-example-32x40.png 32w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-example-64x80.png 64w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-example-24x30.png 24w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-example-48x60.png 48w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-example-19x24.png 19w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-example-39x48.png 39w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-example-15x18.png 15w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Salience-score-example-29x36.png 29w\" sizes=\"(max-width: 630px) 100vw, 630px\" \/>\n<p>This is because the AI has identified that Yorkshire is more closely related to the main topic of the article than West Indies.<\/p>\n<p>This also ties to Google\u2019s indexing infrastructure, which we know makes use of an inverted index and vectorisation to organise content in an efficient manner for easier retrieval.<\/p>\n<p>If we imagine the index as being broken down into different entities.<\/p>\n<p>Google pulls all of the content from a URL and describes it as a vector which includes certain characteristics of the page.<\/p>\n<p>One of these key value pairs within the matrix describes the entities identified within the page and each of these, will be defined by their own vectors of key-value pairs. These include a salience score for each entity, as well as connected entities.<\/p>\n<p>For example, if we looked at an article on Google, one of the entities may be it\u2019s address (Mountain View, CA) and other characteristics such as when the company was founded, number of employees, turnover, profit etc would also be assigned their own salience score.<\/p>\n<p>Here\u2019s a JSON representation of <a href=\"https:\/\/cloud.google.com\/natural-language\/docs\/reference\/rest\/v1\/Entity\" target=\"_blank\" rel=\"nofollow noopener\">how Google stores information about an Entity<\/a> from one of its Cloud Docs :<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1446\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-JSON.png\" alt=\"\" width=\"460\" height=\"770\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-JSON.png 460w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-JSON-24x40.png 24w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-JSON-48x80.png 48w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-JSON-18x30.png 18w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-JSON-36x60.png 36w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-JSON-14x24.png 14w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-JSON-29x48.png 29w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-JSON-11x18.png 11w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-JSON-22x36.png 22w\" sizes=\"(max-width: 460px) 100vw, 460px\" \/>\n<p>In the above example, we can see that Google also keeps track of the number of mentions, a factor that we\u2019ve already covered previously in this section.<\/p>\n<p>RankBrain takes these vectors and compares the entities to determine whether they are relevant to each other.<\/p>\n<p>In cases where there is no entity to match (i.e. it\u2019s a query Google hasn\u2019t seen before), the algorithm calculates the proximity of the closest entity that it can find and concludes that this is what the user is looking for.<\/p>\n<h4>Applications of Entity Salience in SEO<\/h4>\n<p>Although Google\u2019s John Mueller has pretty much debunked entity salience as an important factor (see a tweet from him below), the fact that we know Google are assigning salience scores to entities and are looking at salience in order to understand language, implies that there is some value in optimising your content with entity salience in mind.<\/p>\n<blockquote class=\"twitter-tweet\">\n<p dir=\"ltr\" lang=\"en\">Where do you see that?<\/p>\n<p>\u2014 \ud83c\udf4c John \ud83c\udf4c (@JohnMu) <a href=\"https:\/\/twitter.com\/JohnMu\/status\/1205833500617838592?ref_src=twsrc%5Etfw\">December 14, 2019<\/a><\/p><\/blockquote>\n<p><script async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"><\/script><\/p>\n<p>Moreover, <a href=\"https:\/\/cloud.google.com\/natural-language\/docs\" target=\"_blank\" rel=\"nofollow noopener\">Google\u2019s Natural Language API<\/a> page explicitly refers to Google Search in the following statement: <em>\u201cThe Natural Language API offers you the same deep machine learning technology that powers both Google Search\u2019s ability to answer specific user questions and the language-understanding system behind Google Assistant.\u201d<\/em><\/p>\n<p>By sharing this demo publically, we have a glimpse into how Google understands text, and as a result, we can optimise our text to make it easier for Google to do this.<\/p>\n<p>So, let\u2019s take a look at how you can apply the magic of entity salience within your content.<\/p>\n<p>1. <strong>Text Position and Grammatical Function<\/strong> &#8211; you can use the ordering of words as well as grammatical tricks to boost the salience of entities that you want to target.<\/p>\n<p>Below, we can see how the positioning of the words impacts the salience score that Google assigns to the entities.<\/p>\n<p>Sentence 1: Frodo took the ring to Mordor.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1447\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Entity-salience-example-1.png\" alt=\"\" width=\"1002\" height=\"406\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Entity-salience-example-1.png 1002w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Entity-salience-example-1-99x40.png 99w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Entity-salience-example-1-197x80.png 197w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Entity-salience-example-1-74x30.png 74w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Entity-salience-example-1-148x60.png 148w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Entity-salience-example-1-59x24.png 59w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Entity-salience-example-1-118x48.png 118w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Entity-salience-example-1-44x18.png 44w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/8-Entity-salience-example-1-89x36.png 89w\" sizes=\"(max-width: 1002px) 100vw, 1002px\" \/>\n<p>Sentence 2: The ring was taken to Mordor by Frodo.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1448\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Entity-salience-example-2.png\" alt=\"\" width=\"1002\" height=\"416\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Entity-salience-example-2.png 1002w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Entity-salience-example-2-96x40.png 96w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Entity-salience-example-2-193x80.png 193w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Entity-salience-example-2-72x30.png 72w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Entity-salience-example-2-145x60.png 145w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Entity-salience-example-2-58x24.png 58w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Entity-salience-example-2-116x48.png 116w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Entity-salience-example-2-43x18.png 43w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Entity-salience-example-2-87x36.png 87w\" sizes=\"(max-width: 1002px) 100vw, 1002px\" \/>\n<p>2.<strong> Linguistic Dependance<\/strong> &#8211; considering that NLP is powerful enough to \u201cread&#8221; each word in a sentence at the same time (as opposed to one at a time), it means that processors like Google are able to understand linguistic relationships between words. We can see this in action by looking at the Syntax tab for the sentence \u201cFrodo took the ring to Mordor\u201d.<\/p>\n<img loading=\"lazy\" class=\"alignnone size-full wp-image-1449\" src=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Syntax-tab.png\" alt=\"\" width=\"1046\" height=\"697\" srcset=\"https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Syntax-tab.png 1046w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Syntax-tab-60x40.png 60w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Syntax-tab-120x80.png 120w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Syntax-tab-45x30.png 45w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Syntax-tab-90x60.png 90w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Syntax-tab-36x24.png 36w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Syntax-tab-72x48.png 72w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Syntax-tab-27x18.png 27w, https:\/\/susodigital.com\/textbook\/wp-content\/uploads\/2020\/08\/Syntax-tab-54x36.png 54w\" sizes=\"(max-width: 1046px) 100vw, 1046px\" \/>\n<p>Google unpicks the relationships between each of these words to find the most salient entities by looking at several grammatical features.<\/p>\n<p>For example, the green arrows depict dependencies between words for their meaning.<\/p>\n<p>3. <strong>Entity References and Mentions<\/strong> &#8211; we have seen that Google keeps a tally of the number of mentions of a particular entity, therefore, a simple way to help increase the salience of an entity that you want to target, is to mention it more often. However, this should be done sparingly by incorporating a mixture of named (i.e. Adil Rashid), nominal (i.e. spinner) and pronominal (i.e. he) references.<\/p>\n<p>It\u2019s worth mentioning that Google is ultimately looking for naturally written content, so although entity salience holds some importance in increasing Google\u2019s understanding of your text, it should be secondary to providing quality content to your audience.<\/p>\n"},"_links":{"self":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/1416"}],"collection":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules"}],"about":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/types\/module"}],"version-history":[{"count":10,"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/1416\/revisions"}],"predecessor-version":[{"id":1450,"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/1416\/revisions\/1450"}],"up":[{"embeddable":true,"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/modules\/408"}],"wp:attachment":[{"href":"https:\/\/susodigital.com\/textbook\/wp-json\/wp\/v2\/media?parent=1416"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}